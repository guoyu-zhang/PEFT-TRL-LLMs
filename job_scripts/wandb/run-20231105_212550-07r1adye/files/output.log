Downloading (…)okenizer_config.json: 100%|██████████| 1.62k/1.62k [00:00<00:00, 3.07MB/s]
Downloading tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 19.4MB/s]
Downloading (…)/main/tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 3.80MB/s]
Downloading (…)cial_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 414kB/s]
Downloading (…)lve/main/config.json: 100%|██████████| 614/614 [00:00<00:00, 627kB/s]
Downloading (…)fetensors.index.json: 100%|██████████| 26.8k/26.8k [00:00<00:00, 24.3MB/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]





















































































Downloading shards:  50%|█████     | 1/2 [02:52<02:52, 172.71s/it][02:52<00:00, 79.4MB/s]








































Downloading shards: 100%|██████████| 2/2 [04:10<00:00, 125.39s/it][01:17<00:00, 15.6MB/s]

Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.05s/it]
Downloading (…)neration_config.json: 100%|██████████| 188/188 [00:00<00:00, 210kB/s]






























































































Map: 100%|██████████| 112052/112052 [03:09<00:00, 592.67 examples/s]










Map: 100%|██████████| 12451/12451 [00:20<00:00, 602.73 examples/s]
DatasetDict({
    train: Dataset({
        features: ['prompt', 'response', 'chosen', 'rejected', 'input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected'],
        num_rows: 112052
    })
    test: Dataset({
        features: ['prompt', 'response', 'chosen', 'rejected', 'input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected'],
        num_rows: 12451
    })
})