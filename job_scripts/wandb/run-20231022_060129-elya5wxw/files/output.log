
If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`
/exports/eddie/scratch/s1808795/PEFT-TRL-LLMs/venv/lib/python3.11/site-packages/trl/trainer/reward_trainer.py:105: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
/exports/eddie/scratch/s1808795/PEFT-TRL-LLMs/venv/lib/python3.11/site-packages/trl/trainer/reward_trainer.py:150: UserWarning: When using RewardDataCollatorWithPadding, you should set `max_length` in RewardConfig. It will be set to `512` by default, but you should do it yourself in the future.
  warnings.warn(
/exports/eddie/scratch/s1808795/PEFT-TRL-LLMs/venv/lib/python3.11/site-packages/trl/trainer/reward_trainer.py:175: UserWarning: When using RewardDataCollatorWithPadding, you should set `remove_unused_columns=False` in your RewardConfig we have set it for you, but you should do it yourself in the future.
  warnings.warn(
  0%|          | 0/14007 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/exports/eddie/scratch/s1808795/PEFT-TRL-LLMs/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2640: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed

  0%|          | 9/14007 [00:14<1:54:42,  2.03it/s]

  0%|          | 15/14007 [00:16<1:13:30,  3.17it/s]


  0%|          | 30/14007 [00:20<1:07:14,  3.46it/s]

  0%|          | 36/14007 [00:22<1:04:45,  3.60it/s]

  0%|          | 44/14007 [00:24<1:04:50,  3.59it/s]


  0%|          | 57/14007 [00:28<1:05:50,  3.53it/s]

  0%|          | 64/14007 [00:30<1:09:36,  3.34it/s]


  1%|          | 78/14007 [00:34<1:07:01,  3.46it/s]

  1%|          | 85/14007 [00:36<1:04:03,  3.62it/s]


  1%|          | 100/14007 [00:40<1:03:43,  3.64it/s]

  1%|          | 107/14007 [00:42<1:05:14,  3.55it/s]


  1%|          | 120/14007 [00:46<1:04:26,  3.59it/s]

  1%|          | 127/14007 [00:48<1:09:38,  3.32it/s]

  1%|          | 135/14007 [00:50<1:04:08,  3.60it/s]


  1%|          | 149/14007 [00:54<1:03:41,  3.63it/s]

  1%|          | 155/14007 [00:56<1:05:15,  3.54it/s]

  1%|          | 162/14007 [00:58<1:08:48,  3.35it/s]


  1%|▏         | 176/14007 [01:02<1:03:42,  3.62it/s]

  1%|▏         | 184/14007 [01:04<1:04:40,  3.56it/s]


  1%|▏         | 198/14007 [01:08<1:03:28,  3.63it/s]

  1%|▏         | 204/14007 [01:10<1:06:11,  3.48it/s]


  2%|▏         | 218/14007 [01:14<1:04:28,  3.56it/s]

  2%|▏         | 225/14007 [01:16<1:11:28,  3.21it/s]


  2%|▏         | 240/14007 [01:20<1:05:17,  3.51it/s]

  2%|▏         | 247/14007 [01:22<1:03:22,  3.62it/s]

  2%|▏         | 254/14007 [01:24<1:07:18,  3.41it/s]


  2%|▏         | 268/14007 [01:28<1:04:13,  3.57it/s]

  2%|▏         | 275/14007 [01:30<1:03:54,  3.58it/s]


  2%|▏         | 288/14007 [01:34<1:09:52,  3.27it/s]

  2%|▏         | 296/14007 [01:36<1:03:13,  3.61it/s]


  2%|▏         | 310/14007 [01:40<1:03:08,  3.62it/s]

  2%|▏         | 317/14007 [01:42<1:03:49,  3.57it/s]


  2%|▏         | 331/14007 [01:46<1:05:24,  3.48it/s]

  2%|▏         | 338/14007 [01:48<1:09:02,  3.30it/s]

  2%|▏         | 345/14007 [01:50<1:03:15,  3.60it/s]


  3%|▎         | 359/14007 [01:54<1:02:57,  3.61it/s]

  3%|▎         | 366/14007 [01:56<1:04:24,  3.53it/s]


  3%|▎         | 380/14007 [02:00<1:04:01,  3.55it/s]

  3%|▎         | 387/14007 [02:02<1:07:31,  3.36it/s]

  3%|▎         | 394/14007 [02:04<1:03:07,  3.59it/s]


  3%|▎         | 408/14007 [02:08<1:03:21,  3.58it/s]

  3%|▎         | 415/14007 [02:10<1:05:29,  3.46it/s]


  3%|▎         | 429/14007 [02:14<1:05:13,  3.47it/s]

  3%|▎         | 436/14007 [02:16<1:02:30,  3.62it/s]

  3%|▎         | 443/14007 [02:18<1:03:04,  3.58it/s]


  3%|▎         | 457/14007 [02:22<1:03:11,  3.57it/s]

  3%|▎         | 464/14007 [02:24<1:06:06,  3.41it/s]


  3%|▎         | 478/14007 [02:28<1:07:45,  3.33it/s]

  3%|▎         | 485/14007 [02:30<1:02:54,  3.58it/s]

  4%|▎         | 492/14007 [02:32<1:04:08,  3.51it/s]

  4%|▎         | 500/14007 [02:34<1:01:57,  3.63it/s]/exports/eddie/scratch/s1808795/PEFT-TRL-LLMs/venv/lib/python3.11/site-packages/trl/trainer/reward_trainer.py:256: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3614.)
  logits = torch.stack(logits).mean(dim=2).softmax(dim=0).T



























































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































  File "/home/s1808795/git/PEFT-TRL-LLMs/python_files/reward_model.py", line 83, in <module>
    trainer.train()
  File "/exports/eddie/scratch/s1808795/PEFT-TRL-LLMs/venv/lib/python3.11/site-packages/transformers/trainer.py", line 1591, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/exports/eddie/scratch/s1808795/PEFT-TRL-LLMs/venv/lib/python3.11/site-packages/transformers/trainer.py", line 1984, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/exports/eddie/scratch/s1808795/PEFT-TRL-LLMs/venv/lib/python3.11/site-packages/transformers/trainer.py", line 2328, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/exports/eddie/scratch/s1808795/PEFT-TRL-LLMs/venv/lib/python3.11/site-packages/transformers/trainer.py", line 3066, in evaluate
    output = eval_loop(
             ^^^^^^^^^^
  File "/exports/eddie/scratch/s1808795/PEFT-TRL-LLMs/venv/lib/python3.11/site-packages/transformers/trainer.py", line 3359, in evaluation_loop
    metrics = self.compute_metrics(EvalPrediction(predictions=all_preds, label_ids=all_labels))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/exports/eddie/scratch/s1808795/PEFT-TRL-LLMs/venv/lib/python3.11/site-packages/trl/trainer/utils.py", line 668, in compute_accuracy
    accuracy = np.array(predictions == labels, dtype=float).mean().item()
                        ^^^^^^^^^^^^^^^^^^^^^
ValueError: operands could not be broadcast together with shapes (94984467,2) (94984467,)