Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 26.3kB/s]
Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 713kB/s]
Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 14.5MB/s]
Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 5.89MB/s]

Downloading model.safetensors: 100%|██████████| 440M/440M [00:03<00:00, 134MB/s]
If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`
Downloading readme: 100%|██████████| 478/478 [00:00<00:00, 1.31MB/s]
Downloading metadata: 100%|██████████| 930/930 [00:00<00:00, 1.63MB/s]
Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]








Downloading data files:  50%|█████     | 1/2 [00:21<00:21, 21.02s/it]

Downloading data files: 100%|██████████| 2/2 [00:23<00:00, 11.72s/it]]
Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 299.96it/s]
Generating train split: 100%|██████████| 112052/112052 [00:00<00:00, 124157.25 examples/s]
Generating test split: 100%|██████████| 12451/12451 [00:00<00:00, 167875.71 examples/s]






























































































Map: 100%|██████████| 112052/112052 [03:08<00:00, 594.18 examples/s]










Map: 100%|██████████| 12451/12451 [00:21<00:00, 575.74 examples/s]
/exports/eddie/scratch/s1808795/PEFT-TRL-LLMs/venv/lib/python3.11/site-packages/trl/trainer/reward_trainer.py:105: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
/exports/eddie/scratch/s1808795/PEFT-TRL-LLMs/venv/lib/python3.11/site-packages/trl/trainer/reward_trainer.py:150: UserWarning: When using RewardDataCollatorWithPadding, you should set `max_length` in RewardConfig. It will be set to `512` by default, but you should do it yourself in the future.
  warnings.warn(
/exports/eddie/scratch/s1808795/PEFT-TRL-LLMs/venv/lib/python3.11/site-packages/trl/trainer/reward_trainer.py:175: UserWarning: When using RewardDataCollatorWithPadding, you should set `remove_unused_columns=False` in your RewardConfig we have set it for you, but you should do it yourself in the future.
  warnings.warn(
  0%|          | 0/28013 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/exports/eddie/scratch/s1808795/PEFT-TRL-LLMs/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2640: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed

  0%|          | 19/28013 [00:09<1:22:16,  5.67it/s]

  0%|          | 30/28013 [00:10<1:20:59,  5.76it/s]
{'loss': 1.3493, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}

  0%|          | 42/28013 [00:12<1:20:53,  5.76it/s]

  0%|          | 52/28013 [00:14<1:21:46,  5.70it/s]

  0%|          | 64/28013 [00:16<1:18:51,  5.91it/s]

  0%|          | 75/28013 [00:18<1:21:01,  5.75it/s]

  0%|          | 86/28013 [00:20<1:26:05,  5.41it/s]

  0%|          | 98/28013 [00:22<1:21:15,  5.73it/s]

  0%|          | 109/28013 [00:24<1:23:14,  5.59it/s]

  0%|          | 120/28013 [00:26<1:20:55,  5.75it/s]

  0%|          | 132/28013 [00:29<1:22:32,  5.63it/s]
{'loss': 0.7408, 'learning_rate': 9.99283487980511e-06, 'epoch': 0.0}

  1%|          | 143/28013 [00:30<1:20:04,  5.80it/s]

  1%|          | 155/28013 [00:33<1:22:43,  5.61it/s]

  1%|          | 166/28013 [00:35<1:25:31,  5.43it/s]

  1%|          | 177/28013 [00:36<1:21:03,  5.72it/s]

  1%|          | 189/28013 [00:38<1:18:52,  5.88it/s]

  1%|          | 201/28013 [00:40<1:19:00,  5.87it/s]
{'loss': 0.5982, 'learning_rate': 9.96775695912299e-06, 'epoch': 0.01}

  1%|          | 212/28013 [00:43<1:22:26,  5.62it/s]

  1%|          | 224/28013 [00:45<1:19:48,  5.80it/s]

  1%|          | 235/28013 [00:46<1:19:27,  5.83it/s]

  1%|          | 247/28013 [00:49<1:25:38,  5.40it/s]

  1%|          | 259/28013 [00:51<1:18:48,  5.87it/s]

  1%|          | 270/28013 [00:52<1:18:31,  5.89it/s]
{'loss': 0.7497, 'learning_rate': 9.94267903844087e-06, 'epoch': 0.01}

  1%|          | 282/28013 [00:54<1:19:23,  5.82it/s]

  1%|          | 293/28013 [00:57<1:24:14,  5.48it/s]

  1%|          | 304/28013 [00:58<1:19:36,  5.80it/s]

  1%|          | 316/28013 [01:00<1:18:08,  5.91it/s]

  1%|          | 328/28013 [01:03<1:19:52,  5.78it/s]

  1%|          | 339/28013 [01:04<1:18:12,  5.90it/s]

  1%|▏         | 351/28013 [01:07<1:18:16,  5.89it/s]
{'loss': 0.669, 'learning_rate': 9.914018557661306e-06, 'epoch': 0.01}

  1%|▏         | 363/28013 [01:09<1:20:31,  5.72it/s]

  1%|▏         | 374/28013 [01:11<1:22:05,  5.61it/s]

  1%|▏         | 386/28013 [01:13<1:17:46,  5.92it/s]

  1%|▏         | 397/28013 [01:14<1:19:15,  5.81it/s]

  1%|▏         | 409/28013 [01:16<1:17:42,  5.92it/s]

  1%|▏         | 420/28013 [01:18<1:18:45,  5.84it/s]

  2%|▏         | 432/28013 [01:21<1:19:12,  5.80it/s]
{'loss': 0.5814, 'learning_rate': 9.88535807688174e-06, 'epoch': 0.01}

  2%|▏         | 444/28013 [01:23<1:18:26,  5.86it/s]

  2%|▏         | 455/28013 [01:25<1:25:29,  5.37it/s]

  2%|▏         | 467/28013 [01:27<1:18:07,  5.88it/s]

  2%|▏         | 479/28013 [01:29<1:18:21,  5.86it/s]

  2%|▏         | 490/28013 [01:31<1:17:41,  5.90it/s]
  2%|▏         | 500/28013 [01:32<1:18:38,  5.83it/s]/exports/eddie/scratch/s1808795/PEFT-TRL-LLMs/venv/lib/python3.11/site-packages/trl/trainer/reward_trainer.py:256: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3614.)
  logits = torch.stack(logits).mean(dim=2).softmax(dim=0).T
  0%|          | 3/6226 [00:00<04:23, 23.59it/s]
{'loss': 0.6823, 'learning_rate': 9.860280156199621e-06, 'epoch': 0.02}
































































































































































































































































































































































































































































































































































































































































































































































































































































































