
If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`










Map: 100%|██████████| 12451/12451 [00:21<00:00, 584.47 examples/s]
/exports/eddie/scratch/s1808795/PEFT-TRL-LLMs/venv/lib/python3.11/site-packages/trl/trainer/reward_trainer.py:105: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
/exports/eddie/scratch/s1808795/PEFT-TRL-LLMs/venv/lib/python3.11/site-packages/trl/trainer/reward_trainer.py:150: UserWarning: When using RewardDataCollatorWithPadding, you should set `max_length` in RewardConfig. It will be set to `512` by default, but you should do it yourself in the future.
  warnings.warn(
/exports/eddie/scratch/s1808795/PEFT-TRL-LLMs/venv/lib/python3.11/site-packages/trl/trainer/reward_trainer.py:175: UserWarning: When using RewardDataCollatorWithPadding, you should set `remove_unused_columns=False` in your RewardConfig we have set it for you, but you should do it yourself in the future.
  warnings.warn(
  0%|          | 0/28013 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/exports/eddie/scratch/s1808795/PEFT-TRL-LLMs/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2640: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed


  0%|          | 14/28013 [00:08<2:47:37,  2.78it/s]

  0%|          | 20/28013 [00:10<2:44:27,  2.84it/s]


  0%|          | 31/28013 [00:14<2:43:19,  2.86it/s]


  0%|          | 42/28013 [00:18<2:43:28,  2.85it/s]


  0%|          | 54/28013 [00:22<2:44:38,  2.83it/s]


  0%|          | 65/28013 [00:26<2:43:25,  2.85it/s]

  0%|          | 71/28013 [00:28<2:44:13,  2.84it/s]


  0%|          | 82/28013 [00:32<2:43:23,  2.85it/s]


  0%|          | 94/28013 [00:36<2:40:09,  2.91it/s]


  0%|          | 105/28013 [00:40<2:43:21,  2.85it/s]

  0%|          | 111/28013 [00:42<2:42:50,  2.86it/s]


  0%|          | 122/28013 [00:46<2:44:34,  2.82it/s]


  0%|          | 133/28013 [00:50<2:44:19,  2.83it/s]


  1%|          | 145/28013 [00:54<2:43:02,  2.85it/s]

  1%|          | 150/28013 [00:56<2:43:23,  2.84it/s]


  1%|          | 162/28013 [01:00<2:42:19,  2.86it/s]


  1%|          | 173/28013 [01:04<2:43:38,  2.84it/s]


  1%|          | 185/28013 [01:08<2:42:46,  2.85it/s]

  1%|          | 190/28013 [01:10<2:43:47,  2.83it/s]


  1%|          | 202/28013 [01:14<2:42:33,  2.85it/s]


  1%|          | 213/28013 [01:18<2:43:52,  2.83it/s]


  1%|          | 224/28013 [01:22<2:43:32,  2.83it/s]

  1%|          | 230/28013 [01:24<2:42:36,  2.85it/s]


  1%|          | 241/28013 [01:28<2:43:09,  2.84it/s]


  1%|          | 253/28013 [01:32<2:40:50,  2.88it/s]


  1%|          | 264/28013 [01:36<2:41:25,  2.86it/s]

  1%|          | 270/28013 [01:38<2:42:00,  2.85it/s]


  1%|          | 281/28013 [01:42<2:42:14,  2.85it/s]


  1%|          | 293/28013 [01:46<2:42:08,  2.85it/s]


  1%|          | 304/28013 [01:50<2:42:56,  2.83it/s]

  1%|          | 310/28013 [01:52<2:41:03,  2.87it/s]


  1%|          | 321/28013 [01:56<2:41:59,  2.85it/s]


  1%|          | 333/28013 [02:00<2:41:55,  2.85it/s]


  1%|          | 344/28013 [02:04<2:41:42,  2.85it/s]

  1%|          | 350/28013 [02:06<2:42:13,  2.84it/s]


  1%|▏         | 361/28013 [02:10<2:43:15,  2.82it/s]


  1%|▏         | 372/28013 [02:14<2:46:53,  2.76it/s]


  1%|▏         | 384/28013 [02:18<2:42:41,  2.83it/s]


  1%|▏         | 395/28013 [02:22<2:41:38,  2.85it/s]

  1%|▏         | 401/28013 [02:24<2:41:54,  2.84it/s]


  1%|▏         | 412/28013 [02:28<2:42:47,  2.83it/s]


  2%|▏         | 424/28013 [02:32<2:41:14,  2.85it/s]


  2%|▏         | 435/28013 [02:36<2:42:11,  2.83it/s]

  2%|▏         | 441/28013 [02:38<2:41:33,  2.84it/s]


  2%|▏         | 452/28013 [02:42<2:41:17,  2.85it/s]


  2%|▏         | 464/28013 [02:46<2:41:14,  2.85it/s]


  2%|▏         | 475/28013 [02:50<2:41:10,  2.85it/s]

  2%|▏         | 481/28013 [02:52<2:40:39,  2.86it/s]


  2%|▏         | 492/28013 [02:56<2:41:37,  2.84it/s]

  2%|▏         | 500/28013 [02:59<2:41:40,  2.84it/s]/exports/eddie/scratch/s1808795/PEFT-TRL-LLMs/venv/lib/python3.11/site-packages/trl/trainer/reward_trainer.py:256: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3614.)
  logits = torch.stack(logits).mean(dim=2).softmax(dim=0).T
  0%|          | 11/6226 [00:01<12:43,  8.14it/s]
































































































































































































































































































































































































































































































































































































































































































































































































































































































































































